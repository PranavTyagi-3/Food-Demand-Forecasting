{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "#from gain import GAIN\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# from missingpy import MissForest\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy import stats\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer, MaxAbsScaler, Normalizer, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder, HelmertEncoder, SumEncoder, BackwardDifferenceEncoder, LeaveOneOutEncoder, JamesSteinEncoder, BinaryEncoder\n",
    "\n",
    "import sys\n",
    "import dvc.api\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_type=\"\"\n",
    "missing_option=\"\"\n",
    "categorical_option=\"\"\n",
    "scaling_option=\"\"\n",
    "\n",
    "with open('D:/Study/INTERNSHIP/FINAL/params.json','r') as f:\n",
    "    di=json.load(f)\n",
    "    model_type=di['model_type']\n",
    "    missing_option=di['null_values']\n",
    "    categorical_option=di['encoding']\n",
    "    scaling_option=di['scaling']\n",
    "    \n",
    "def get_categorical_columns(dataset):\n",
    "    # Assuming categorical columns have 'object' data type, you can adjust the condition based on your dataset\n",
    "    print(dataset.info())\n",
    "    return list(dataset.select_dtypes(include=['object']).columns)\n",
    "\n",
    "def transform(dataset, missing_option, categorical_option, scaling_option):\n",
    "    print(missing_option, categorical_option, scaling_option)\n",
    "    # Handling missing values\n",
    "    '''\n",
    "    print(\"\\nHandling Missing Values:\")\n",
    "    print(\"1. Mean Imputation\")\n",
    "    print(\"2. Median Imputation\")\n",
    "    print(\"3. Custom Value Imputation\")\n",
    "    print(\"4. Most Frequent Imputation\")\n",
    "    print(\"5. KNN Imputation\")\n",
    "    print(\"6. Linear Regression Imputation\")\n",
    "    print(\"7. Iterative Imputation\")\n",
    "    print(\"8. Multiple Imputation by Chained Equations (MICE)\")\n",
    "    print(\"9 Autoencoder Imputation\")\n",
    "    '''\n",
    "    missing_columns = dataset.columns[dataset.isnull().any()].tolist()\n",
    "    \n",
    "    if missing_columns:\n",
    "        \n",
    "        if missing_option == \"Mean\":\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "        elif missing_option == \"Median\":\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "        elif missing_option == \"Mode\":\n",
    "            imputer = SimpleImputer(strategy='most_frequent')\n",
    "        elif missing_option == \"Linear Regression Imputation\":\n",
    "            imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "        \n",
    "        dataset[missing_columns] = imputer.fit_transform(dataset[missing_columns])\n",
    "    \n",
    "    # Encoding categorical data\n",
    "    '''print(\"\\nEncoding Categorical Data:\")\n",
    "    print(\"1. One-Hot Encoding\")\n",
    "    print(\"2. Label Encoding\")\n",
    "    print(\"3. Target Encoding\")\n",
    "    print(\"4. Helmert Coding\")\n",
    "    print(\"5. Sum Coding\")\n",
    "    print(\"6. Backward Difference Coding\")\n",
    "    print(\"7. Leave-One-Out Encoding\")\n",
    "    print(\"8. James-Stein Encoder\")\n",
    "    print(\"9. Binary Encoding\")'''\n",
    "    \n",
    "    categorical_columns = get_categorical_columns(dataset)\n",
    "    if categorical_option=='None':\n",
    "        print(\"None\")\n",
    "\n",
    "    elif categorical_columns and categorical_option == 'One Hot Encoding':\n",
    "        encoder = OneHotEncoder()\n",
    "        print(dataset.iloc[0])\n",
    "        dataset = pd.DataFrame(encoder.fit_transform(dataset[categorical_columns]).toarray(), columns=encoder.get_feature_names_out(categorical_columns))\n",
    "        \n",
    "    elif categorical_columns and categorical_option == 'Label Encoding':\n",
    "        dataset[categorical_columns] = dataset[categorical_columns].astype('category')\n",
    "        for column in categorical_columns:\n",
    "            dataset[column] = dataset[column].cat.codes\n",
    "       \n",
    "    elif categorical_columns and categorical_option == 'Helmert Encoding':\n",
    "        encoder = HelmertEncoder(cols=categorical_columns)\n",
    "        dataset[categorical_columns] = encoder.fit_transform(dataset[categorical_columns])\n",
    "    \n",
    "    elif categorical_columns and categorical_option == 'Sum Encoding':\n",
    "        encoder = SumEncoder(cols=categorical_columns)\n",
    "        dataset[categorical_columns] = encoder.fit_transform(dataset[categorical_columns])\n",
    "    \n",
    "    elif categorical_columns and categorical_option == 'Backward Difference Encoding':\n",
    "        encoder = BackwardDifferenceEncoder(cols=categorical_columns)\n",
    "        dataset[categorical_columns] = encoder.fit_transform(dataset[categorical_columns])\n",
    "         \n",
    "    elif categorical_columns and categorical_option == 'James-Stein Encoder':\n",
    "        encoder = JamesSteinEncoder(cols=categorical_columns)\n",
    "        dataset[categorical_columns] = encoder.fit_transform(dataset[categorical_columns])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid input for encoding categorical data!!\")\n",
    "    \n",
    "    # Feature scaling\n",
    "    '''print(\"\\nFeature Scaling:\")\n",
    "    print(\"1. Standard Scaling (Z-score)\")\n",
    "    print(\"2. Min-Max Scaling\")\n",
    "    print(\"3. Robust Scaling\")\n",
    "    print(\"4. Power Transformation (Yeo-Johnson)\")\n",
    "    print(\"5. Quantile Transformation\")\n",
    "    print(\"6. MaxAbsScaler\")\n",
    "    print(\"7. Normalizer\")'''\n",
    "    numerical_columns = list(set(dataset.columns) - set(categorical_columns))\n",
    "    print(numerical_columns)\n",
    "    print(categorical_columns)\n",
    "    if scaling_option == 'Standard Scaling':\n",
    "        scaler = StandardScaler(with_mean=False)  # Pass with_mean=False for sparse matrices\n",
    "        dataset.iloc[:, :] = scaler.fit_transform(dataset)\n",
    "\n",
    "    elif scaling_option == 'Min-Max Scaling':\n",
    "        scaler = MinMaxScaler()\n",
    "        dataset.iloc[:, :] = scaler.fit_transform(dataset)\n",
    "\n",
    "    elif scaling_option == 'Robust Scaling':\n",
    "        scaler = RobustScaler()\n",
    "        dataset.iloc[:, :] = scaler.fit_transform(dataset)\n",
    "\n",
    "    elif scaling_option == 'Power Transformation':\n",
    "        scaler = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "        dataset.iloc[:, :] = scaler.fit_transform(dataset)\n",
    "\n",
    "    elif scaling_option == 'Quantile Transformation':\n",
    "        scaler = QuantileTransformer(output_distribution='uniform')\n",
    "        dataset.iloc[:, :] = scaler.fit_transform(dataset)\n",
    "\n",
    "    elif scaling_option == 'MaxAbsScaler':\n",
    "        scaler = MaxAbsScaler()\n",
    "        dataset.iloc[:, :] = scaler.fit_transform(dataset)\n",
    "\n",
    "    elif scaling_option == 'Normalizer':\n",
    "        scaler = Normalizer()\n",
    "        dataset.iloc[:, :] = scaler.fit_transform(dataset)\n",
    "\n",
    "    else:\n",
    "        print(scaling_option)\n",
    "        raise ValueError(\"Invalid input for feature scaling technique.\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def pretime(train_df):\n",
    "\n",
    "  period = len(train_df)\n",
    "  train_df['Date'] = pd.date_range('2015-01-08', periods=period, freq='W')\n",
    "  train_df['Day'] = train_df['Date'].dt.day\n",
    "  train_df['Month'] = train_df['Date'].dt.month\n",
    "  train_df['Year'] = train_df['Date'].dt.year\n",
    "  train_df['Quarter'] = train_df['Date'].dt.quarter\n",
    "\n",
    "  return train_df\n",
    "\n",
    "\n",
    "######################## Main Code Starts ################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#dir path\n",
    "raw_data_path = 'D:/Study/INTERNSHIP/FINAL/data/raw/'\n",
    "processed_data_path = 'D:Study/INTERNSHIP/FINAL/data/processed/preprocessed_data.csv'\n",
    "\n",
    "merged_df = pd.read_csv(raw_data_path + 'train1.csv')\n",
    "\n",
    "#transforms merged_df\n",
    "merged_df.drop(columns=['city_code','region_code','center_type','op_area'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>week</th>\n",
       "      <th>center_id</th>\n",
       "      <th>meal_id</th>\n",
       "      <th>category</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>checkout_price</th>\n",
       "      <th>base_price</th>\n",
       "      <th>emailer_for_promotion</th>\n",
       "      <th>homepage_featured</th>\n",
       "      <th>num_orders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"id\": 1379560</td>\n",
       "      <td>\"week\": 1</td>\n",
       "      <td>\"center_id\": 55</td>\n",
       "      <td>\"meal_id\": 1885</td>\n",
       "      <td>\"category\": \"Beverages\"</td>\n",
       "      <td>\"cuisine\": \"Thai\"</td>\n",
       "      <td>\"checkout_price\": 136.83</td>\n",
       "      <td>\"base_price\": 152.29</td>\n",
       "      <td>\"emailer_for_promotion\": 0</td>\n",
       "      <td>\"homepage_featured\": 0</td>\n",
       "      <td>\"num_orders\": 177}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"id\": 1018704</td>\n",
       "      <td>\"week\": 2</td>\n",
       "      <td>\"center_id\": 55</td>\n",
       "      <td>\"meal_id\": 1885</td>\n",
       "      <td>\"category\": \"Beverages\"</td>\n",
       "      <td>\"cuisine\": \"Thai\"</td>\n",
       "      <td>\"checkout_price\": 135.83</td>\n",
       "      <td>\"base_price\": 152.29</td>\n",
       "      <td>\"emailer_for_promotion\": 0</td>\n",
       "      <td>\"homepage_featured\": 0</td>\n",
       "      <td>\"num_orders\": 323}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"id\": 1196273</td>\n",
       "      <td>\"week\": 3</td>\n",
       "      <td>\"center_id\": 55</td>\n",
       "      <td>\"meal_id\": 1885</td>\n",
       "      <td>\"category\": \"Beverages\"</td>\n",
       "      <td>\"cuisine\": \"Thai\"</td>\n",
       "      <td>\"checkout_price\": 132.92</td>\n",
       "      <td>\"base_price\": 133.92</td>\n",
       "      <td>\"emailer_for_promotion\": 0</td>\n",
       "      <td>\"homepage_featured\": 0</td>\n",
       "      <td>\"num_orders\": 96}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"id\": 1116527</td>\n",
       "      <td>\"week\": 4</td>\n",
       "      <td>\"center_id\": 55</td>\n",
       "      <td>\"meal_id\": 1885</td>\n",
       "      <td>\"category\": \"Beverages\"</td>\n",
       "      <td>\"cuisine\": \"Thai\"</td>\n",
       "      <td>\"checkout_price\": 135.86</td>\n",
       "      <td>\"base_price\": 134.86</td>\n",
       "      <td>\"emailer_for_promotion\": 0</td>\n",
       "      <td>\"homepage_featured\": 0</td>\n",
       "      <td>\"num_orders\": 163}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"id\": 1343872</td>\n",
       "      <td>\"week\": 5</td>\n",
       "      <td>\"center_id\": 55</td>\n",
       "      <td>\"meal_id\": 1885</td>\n",
       "      <td>\"category\": \"Beverages\"</td>\n",
       "      <td>\"cuisine\": \"Thai\"</td>\n",
       "      <td>\"checkout_price\": 146.5</td>\n",
       "      <td>\"base_price\": 147.5</td>\n",
       "      <td>\"emailer_for_promotion\": 0</td>\n",
       "      <td>\"homepage_featured\": 0</td>\n",
       "      <td>\"num_orders\": 215}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id        week         center_id           meal_id  \\\n",
       "0  {\"id\": 1379560   \"week\": 1   \"center_id\": 55   \"meal_id\": 1885   \n",
       "1  {\"id\": 1018704   \"week\": 2   \"center_id\": 55   \"meal_id\": 1885   \n",
       "2  {\"id\": 1196273   \"week\": 3   \"center_id\": 55   \"meal_id\": 1885   \n",
       "3  {\"id\": 1116527   \"week\": 4   \"center_id\": 55   \"meal_id\": 1885   \n",
       "4  {\"id\": 1343872   \"week\": 5   \"center_id\": 55   \"meal_id\": 1885   \n",
       "\n",
       "                   category             cuisine             checkout_price  \\\n",
       "0   \"category\": \"Beverages\"   \"cuisine\": \"Thai\"   \"checkout_price\": 136.83   \n",
       "1   \"category\": \"Beverages\"   \"cuisine\": \"Thai\"   \"checkout_price\": 135.83   \n",
       "2   \"category\": \"Beverages\"   \"cuisine\": \"Thai\"   \"checkout_price\": 132.92   \n",
       "3   \"category\": \"Beverages\"   \"cuisine\": \"Thai\"   \"checkout_price\": 135.86   \n",
       "4   \"category\": \"Beverages\"   \"cuisine\": \"Thai\"    \"checkout_price\": 146.5   \n",
       "\n",
       "              base_price        emailer_for_promotion  \\\n",
       "0   \"base_price\": 152.29   \"emailer_for_promotion\": 0   \n",
       "1   \"base_price\": 152.29   \"emailer_for_promotion\": 0   \n",
       "2   \"base_price\": 133.92   \"emailer_for_promotion\": 0   \n",
       "3   \"base_price\": 134.86   \"emailer_for_promotion\": 0   \n",
       "4    \"base_price\": 147.5   \"emailer_for_promotion\": 0   \n",
       "\n",
       "         homepage_featured           num_orders  \n",
       "0   \"homepage_featured\": 0   \"num_orders\": 177}  \n",
       "1   \"homepage_featured\": 0   \"num_orders\": 323}  \n",
       "2   \"homepage_featured\": 0    \"num_orders\": 96}  \n",
       "3   \"homepage_featured\": 0   \"num_orders\": 163}  \n",
       "4   \"homepage_featured\": 0   \"num_orders\": 215}  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kafka_topic': 'finald', 'kafka_url': 'localhost:9092', 'model_type': 'Regression Models', 'null_values': 'Mean', 'encoding': 'None', 'scaling': 'Standard Scaling', 'model_id': 'Lasso Regression', 'tuning': 'No Tuning', 'db_type': 'sqlite', 'db_name': 'Temp', 'db_url': 'Something'}\n"
     ]
    }
   ],
   "source": [
    "with open('D:/Study/INTERNSHIP/FINAL/params.json','r') as f:\n",
    "    di=json.load(f)\n",
    "    print(di)\n",
    "    topic=di['kafka_topic']\n",
    "    bootstrap_servers=di['kafka_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finald'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
